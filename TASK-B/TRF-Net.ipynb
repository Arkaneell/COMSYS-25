{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb5bb09-670c-4476-96db-9f3d9beb7c73",
   "metadata": {},
   "source": [
    "# TRF-NET\n",
    "#### This is made for Face Recognition derived from ResNet50 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c4a32-828f-459a-956a-da0e891a80f7",
   "metadata": {},
   "source": [
    "## MODULES REQUIRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9de882b5-a58f-457c-b1da-b16d17aeb14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Built-in Modules ===\n",
    "import os                            # For file path management and directory traversal\n",
    "import random                        # For random sampling\n",
    "\n",
    "# === Numerical and Array Handling ===\n",
    "import numpy as np                   # For numerical operations and array handling\n",
    "from collections import defaultdict  # For grouping items by key\n",
    "\n",
    "# === Image Handling ===\n",
    "from PIL import Image                # For image loading and manipulation (optional backup to OpenCV/PIL)\n",
    "import cv2                           # For image reading and resizing (OpenCV)\n",
    "import glob                          # For pattern-based file search (e.g., *.jpg)\n",
    "\n",
    "# === Progress Visualization ===\n",
    "from tqdm import tqdm                # To show progress bars for loops (useful for preprocessing)\n",
    "\n",
    "# === TensorFlow & Keras ===\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses  # Core components\n",
    "from tensorflow.keras.models import Model                        # Model subclassing\n",
    "from tensorflow.keras.layers import Layer                        # Custom layer creation\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array  # Image utilities\n",
    "\n",
    "# === Evaluation Metrics ===\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # To compare embedding vectors\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score  # Classification evaluation metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83961521-d665-4ddc-84a3-d15e5f8e06db",
   "metadata": {},
   "source": [
    "## PREPROCESING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "781a5b61-3df6-4095-b25d-b52957a14576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths_and_labels(root_dir):\n",
    "    \"\"\"\n",
    "    Scans a directory of person-named subfolders and returns:\n",
    "    - List of image file paths\n",
    "    - Corresponding numeric labels\n",
    "    - Mapping of person names to labels\n",
    "    \"\"\"\n",
    "\n",
    "    image_paths = []  # Stores all image file paths (clean + distorted)\n",
    "    labels = []       # Stores corresponding numeric labels\n",
    "    person_names = sorted(os.listdir(root_dir))  # Get all folder names (person identities)\n",
    "\n",
    "    # Map each person name to a unique integer label\n",
    "    person_to_label = {name: idx for idx, name in enumerate(person_names)}\n",
    "\n",
    "    # Loop through each person's folder\n",
    "    for person in person_names:\n",
    "        person_folder = os.path.join(root_dir, person)\n",
    "        if not os.path.isdir(person_folder):\n",
    "            continue  # Skip if not a folder (just in case)\n",
    "\n",
    "        # 1️⃣ Load clean images directly inside the person's folder\n",
    "        clean_images = glob.glob(os.path.join(person_folder, \"*.jpg\"))\n",
    "\n",
    "        # 2️⃣ Load distorted images inside the \"distortion\" subfolder (if it exists)\n",
    "        distortion_folder = os.path.join(person_folder, \"distortion\")\n",
    "        distorted_images = glob.glob(os.path.join(distortion_folder, \"*.jpg\")) if os.path.exists(distortion_folder) else []\n",
    "\n",
    "        # 3️⃣ Combine all image paths (clean + distorted)\n",
    "        all_images = clean_images + distorted_images\n",
    "        label = person_to_label[person]  # Assign label based on person name\n",
    "\n",
    "        # 4️⃣ Store each image and its label\n",
    "        for img in all_images:\n",
    "            image_paths.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "    # Return image paths, labels, and the name-to-label mapping\n",
    "    return image_paths, labels, person_to_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "828e5fe6-6a76-4236-ae1b-1a4e5d88c73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 15408\n",
      "Total unique persons: 878\n"
     ]
    }
   ],
   "source": [
    "# === Define the path to the training dataset folder ===\n",
    "train_dir = \"train\"  # Actual training directory path\n",
    "\n",
    "# === Get all image file paths and corresponding labels ===\n",
    "# - image_paths: list of all image file paths (clean + distorted)\n",
    "# - labels: corresponding integer labels for classification\n",
    "# - class_map: mapping from person names to numeric labels\n",
    "image_paths, labels, class_map = get_image_paths_and_labels(train_dir)\n",
    "\n",
    "# === Display basic dataset statistics ===\n",
    "print(f\"Total images: {len(image_paths)}\")          # Total number of images found\n",
    "print(f\"Total unique persons: {len(class_map)}\")    # Number of distinct person classes (folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59226e9c-8804-4c6d-9d2f-6d3d614da7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] Total images: 3376\n",
      "[val] Total classes: 250\n"
     ]
    }
   ],
   "source": [
    "# === Define the path to the validation dataset folder ===\n",
    "val_dir = \"val\"  # Path to your validation dataset directory\n",
    "\n",
    "# === Extract image paths, labels, and class mapping for the validation set ===\n",
    "# - val_image_paths: List of all image file paths in the val set\n",
    "# - val_labels: Corresponding numeric labels for each image\n",
    "# - val_class_map: Dictionary mapping class names (folder names) to label integers\n",
    "val_image_paths, val_labels, val_class_map = get_image_paths_and_labels(val_dir)\n",
    "\n",
    "# === Print basic statistics of the validation dataset ===\n",
    "print(f\"[val] Total images: {len(val_image_paths)}\")      # Total number of images in val set\n",
    "print(f\"[val] Total classes: {len(val_class_map)}\")       # Number of unique identities (classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a88d4-65fb-4b23-a816-3402bb7d3560",
   "metadata": {},
   "source": [
    "## TRIPLET PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "96d11a3f-b8db-413c-a67e-73b192bf34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_triplets(image_paths, labels):\n",
    "    \"\"\"\n",
    "    Generates triplets for training with triplet loss:\n",
    "    - Anchor: clean image\n",
    "    - Positive: distorted version of the anchor image\n",
    "    - Negative: clean image of a different person\n",
    "\n",
    "    Assumes the following folder structure:\n",
    "    train/\n",
    "        Person_01/\n",
    "            img_01.jpg               ← clean\n",
    "            distortion/\n",
    "                img_01_blur.jpg      ← distorted\n",
    "                img_01_fog.jpg\n",
    "    \"\"\"\n",
    "\n",
    "    # === 1. Group images by label into clean and distorted categories ===\n",
    "    label_to_clean = defaultdict(list)       # Dictionary to store clean images per label\n",
    "    label_to_distorted = defaultdict(list)   # Dictionary to store distorted images per label\n",
    "\n",
    "    for path, label in zip(image_paths, labels):\n",
    "        # Store all image paths initially as clean\n",
    "        label_to_clean[label].append(path)\n",
    "\n",
    "        # Extract folder of the image\n",
    "        img_folder = os.path.dirname(path)\n",
    "        base_name = os.path.splitext(os.path.basename(path))[0]  # e.g., 'img_01'\n",
    "        \n",
    "        # Construct path to potential distortion folder (e.g., train/Person_01/distortion)\n",
    "        distortion_folder = os.path.join(img_folder, 'distortion')\n",
    "\n",
    "        # If distortion folder exists\n",
    "        if os.path.exists(distortion_folder):\n",
    "            # Look for all distorted versions of the current base image\n",
    "            distorted = [\n",
    "                os.path.join(distortion_folder, f)\n",
    "                for f in os.listdir(distortion_folder)\n",
    "                if base_name in f  # Matches pattern like 'img_01_blur.jpg'\n",
    "            ]\n",
    "\n",
    "            # If distorted images found, store them as positive candidates\n",
    "            if distorted:\n",
    "                label_to_distorted[label].append((path, distorted))\n",
    "\n",
    "    # === 2. Generate triplets using the clean/distorted dictionaries ===\n",
    "    triplets = []\n",
    "    label_list = list(label_to_clean.keys())  # All available labels\n",
    "\n",
    "    for label in label_list:\n",
    "        if not label_to_distorted[label]:\n",
    "            continue  # Skip labels that have no distorted images\n",
    "\n",
    "        # For each clean image with at least one distorted match\n",
    "        for clean_img, distorted_list in label_to_distorted[label]:\n",
    "            anchor = clean_img                          # Anchor = clean image\n",
    "            positive = random.choice(distorted_list)    # Positive = one of its distortions\n",
    "\n",
    "            # Pick a label that's different from current label (for negative sample)\n",
    "            negative_label = random.choice([l for l in label_list if l != label])\n",
    "            negative = random.choice(label_to_clean[negative_label])  # Negative = clean image of another person\n",
    "\n",
    "            # Store the triplet\n",
    "            triplets.append((anchor, positive, negative))\n",
    "\n",
    "    return triplets  # Final list of (anchor, positive, negative) image path triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b355c96-9c71-41b2-80a9-34b5bf34b655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example triplet:\n",
      "Anchor: train\\001_frontal\\001_frontal.jpg\n",
      "Positive: train\\001_frontal\\distortion\\001_frontal_rainy.jpg\n",
      "Negative: train\\Robert_Ehrlich\\distortion\\Robert_Ehrlich_0001_resized.jpg\n"
     ]
    }
   ],
   "source": [
    "# === Generate Triplets for Training ===\n",
    "triplets = make_triplets(image_paths, labels)  # Creates (anchor, positive, negative) triplets from the dataset\n",
    "\n",
    "# === Preview a Sample Triplet for Verification ===\n",
    "print(\"Example triplet:\")\n",
    "print(\"Anchor:\", triplets[0][0])    # Path to the clean (anchor) image\n",
    "print(\"Positive:\", triplets[0][1])  # Path to the distorted version of the same person (positive)\n",
    "print(\"Negative:\", triplets[0][2])  # Path to a clean image of a different person (negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10f6f56a-5f25-4827-9965-275cd8b213c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load and preprocess a single image ===\n",
    "def load_image(path):\n",
    "    \"\"\"\n",
    "    Reads and preprocesses an image from the given file path.\n",
    "    - Reads the image as a binary file\n",
    "    - Decodes it as a JPEG image with 3 color channels\n",
    "    - Resizes it to 224x224 (ResNet50 input size)\n",
    "    - Normalizes pixel values to the range [0, 1]\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(path)                            # Read image file as byte string\n",
    "    image = tf.image.decode_jpeg(image, channels=3)          # Decode JPEG to tensor\n",
    "    image = tf.image.resize(image, [224, 224])               # Resize to model input shape\n",
    "    image = tf.cast(image, tf.float32) / 255.0               # Normalize to float32 [0, 1]\n",
    "    return image\n",
    "\n",
    "# === Preprocess an entire triplet of images ===\n",
    "def preprocess_triplet(anchor_path, positive_path, negative_path):\n",
    "    \"\"\"\n",
    "    Given paths to anchor, positive, and negative images,\n",
    "    loads and preprocesses all three into tensors.\n",
    "    Returns a tuple of (anchor, positive, negative) tensors.\n",
    "    \"\"\"\n",
    "    anchor = load_image(anchor_path)\n",
    "    positive = load_image(positive_path)\n",
    "    negative = load_image(negative_path)\n",
    "    return (anchor, positive, negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06701c53-94ee-4bb5-8af7-296e62c6afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplet_dataset(triplets, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow dataset for training with triplet loss.\n",
    "    \n",
    "    Args:\n",
    "        triplets: A list of (anchor_path, positive_path, negative_path) tuples\n",
    "        batch_size: Number of triplets per training batch\n",
    "    \n",
    "    Returns:\n",
    "        A tf.data.Dataset object that yields batches of preprocessed triplets\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the list of triplets into three separate lists: anchors, positives, and negatives\n",
    "    anchor_paths, positive_paths, negative_paths = zip(*triplets)\n",
    "\n",
    "    # Create a dataset of image paths (still as strings)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (list(anchor_paths), list(positive_paths), list(negative_paths))\n",
    "    )\n",
    "\n",
    "    # Map the preprocessing function to load and transform image paths to tensors\n",
    "    dataset = dataset.map(\n",
    "        lambda a, p, n: preprocess_triplet(a, p, n),  # (anchor, positive, negative) tensors\n",
    "        num_parallel_calls=tf.data.AUTOTUNE           # Enables parallel loading\n",
    "    )\n",
    "\n",
    "    # Shuffle the dataset and batch it\n",
    "    dataset = dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9dc85562-e2f4-4a48-a1bc-904aa83c7eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor shape: (32, 224, 224, 3)\n",
      "Positive shape: (32, 224, 224, 3)\n",
      "Negative shape: (32, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# === Create the Triplet Dataset ===\n",
    "triplet_dataset = create_triplet_dataset(triplets, batch_size=32)\n",
    "# This prepares the dataset for training with triplet loss by batching and preprocessing\n",
    "\n",
    "# === Preview One Batch from the Dataset ===\n",
    "for batch in triplet_dataset.take(1):  # Take one batch to inspect\n",
    "    anchor_batch, positive_batch, negative_batch = batch  # Unpack the triplet batch\n",
    "    print(\"Anchor shape:\", anchor_batch.shape)            # Expected: (batch_size, 224, 224, 3)\n",
    "    print(\"Positive shape:\", positive_batch.shape)        # Expected: (batch_size, 224, 224, 3)\n",
    "    print(\"Negative shape:\", negative_batch.shape)        # Expected: (batch_size, 224, 224, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8b2cece-c5d4-4a9d-a5ff-cd442525f259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] Total images: 3376\n",
      "[val] Total classes: 250\n"
     ]
    }
   ],
   "source": [
    "val_dir = \"val\"  # Path to the validation dataset directory\n",
    "\n",
    "# === Get Validation Image Paths and Labels ===\n",
    "# This function returns:\n",
    "# - val_image_paths: list of image file paths\n",
    "# - val_labels: list of corresponding labels (e.g., class/person IDs)\n",
    "# - val_class_map: a dictionary mapping class names to label integers\n",
    "val_image_paths, val_labels, val_class_map = get_image_paths_and_labels(val_dir)\n",
    "\n",
    "# === Print Dataset Statistics ===\n",
    "print(f\"[val] Total images: {len(val_image_paths)}\")  # Number of validation images\n",
    "print(f\"[val] Total classes: {len(val_class_map)}\")   # Number of unique classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3b8daf3-3230-4b03-88b1-5fd2c202d0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 422 triplets from val set\n"
     ]
    }
   ],
   "source": [
    "# === Generate Triplets from Validation Dataset ===\n",
    "val_triplets = make_triplets(val_image_paths, val_labels)\n",
    "# Uses the same logic as training to generate (anchor, positive, negative) triplets from validation set\n",
    "\n",
    "# === Display Number of Triplets Generated ===\n",
    "print(f\"Generated {len(val_triplets)} triplets from val set\")\n",
    "# Helpful for confirming that triplets are formed correctly and there are enough samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "39d28774-5ef5-4c52-a360-09ea2f4ad8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor shape: (32, 224, 224, 3)\n",
      "Positive shape: (32, 224, 224, 3)\n",
      "Negative shape: (32, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# === Create the Triplet Dataset for Validation ===\n",
    "val_triplet_dataset = create_triplet_dataset(triplets, batch_size=32)\n",
    "# This function prepares the validation triplet dataset with preprocessed images\n",
    "# Each element is a batch of (anchor, positive, negative) triplets\n",
    "\n",
    "# === Preview One Batch of Validation Data ===\n",
    "for batch in val_triplet_dataset.take(1):  # Take just one batch for inspection\n",
    "    anchor_batch, positive_batch, negative_batch = batch  # Unpack the batch\n",
    "    print(\"Anchor shape:\", anchor_batch.shape)            # Expected shape: (batch_size, 224, 224, 3)\n",
    "    print(\"Positive shape:\", positive_batch.shape)        # Same as above\n",
    "    print(\"Negative shape:\", negative_batch.shape)        # Same as above\n",
    "\n",
    "# This helps confirm that the dataset pipeline is functioning correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63a08f42-b077-4633-a373-32fea76869cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Add Dummy Labels to Triplet Dataset ===\n",
    "def add_dummy_labels(dataset):\n",
    "    \"\"\"\n",
    "    Adds dummy labels to the triplet dataset.\n",
    "    This is required to satisfy the Keras .fit() function,\n",
    "    which expects both inputs and labels.\n",
    "\n",
    "    Args:\n",
    "        dataset: tf.data.Dataset of (anchor, positive, negative) triplets\n",
    "\n",
    "    Returns:\n",
    "        A new dataset of ((anchor, positive, negative), dummy_label)\n",
    "    \"\"\"\n",
    "    return dataset.map(\n",
    "        lambda a, p, n: ((a, p, n), tf.zeros((tf.shape(a)[0],)))  # Add dummy labels with same batch size\n",
    "    )\n",
    "\n",
    "# === Apply to Validation and Training Sets ===\n",
    "val_triplet_dataset_with_labels = add_dummy_labels(val_triplet_dataset)\n",
    "train_triplet_dataset_with_labels = add_dummy_labels(triplet_dataset)\n",
    "# These datasets now have the structure expected by model.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0bdda9-1fdd-4420-823d-6c3541a3a9d7",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0cf9f0d6-2798-40fd-90c7-fcc22d8a8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(input_tensor, reduction=16):\n",
    "    \"\"\"\n",
    "    Implements a Squeeze-and-Excitation (SE) block.\n",
    "    \n",
    "    Args:\n",
    "        input_tensor: Input feature map (batch, H, W, C)\n",
    "        reduction: Reduction ratio for the intermediate Dense layer (usually 16)\n",
    "    \n",
    "    Returns:\n",
    "        Output tensor after channel-wise reweighting\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get number of channels from the input tensor shape\n",
    "    channels = input_tensor.shape[-1]\n",
    "\n",
    "    # === SQUEEZE ===\n",
    "    # Global Average Pooling across spatial dimensions (H, W) → shape becomes (batch, C)\n",
    "    x = layers.GlobalAveragePooling2D()(input_tensor)\n",
    "\n",
    "    # === EXCITATION ===\n",
    "    # Fully connected layer with reduction to bottleneck the channel dimension\n",
    "    x = layers.Dense(channels // reduction, activation='relu')(x)\n",
    "\n",
    "    # Restore back to original number of channels using sigmoid for weights (between 0 and 1)\n",
    "    x = layers.Dense(channels, activation='sigmoid')(x)\n",
    "\n",
    "    # Reshape to match dimensions for broadcasting → (batch, 1, 1, C)\n",
    "    x = layers.Reshape((1, 1, channels))(x)\n",
    "\n",
    "    # === SCALE ===\n",
    "    # Multiply channel-wise weights with the original input tensor\n",
    "    return layers.Multiply()([input_tensor, x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1361531b-5829-46ea-be93-b37f8d15bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAM(Layer):\n",
    "    def __init__(self, reduction=16, **kwargs):\n",
    "        super(CBAM, self).__init__(**kwargs)\n",
    "        self.reduction = reduction  # Channel reduction ratio for bottleneck MLP\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]  # Number of channels in input\n",
    "\n",
    "        # === Shared MLP for Channel Attention ===\n",
    "        # Applies to both average and max pooled vectors\n",
    "        self.shared_dense = tf.keras.Sequential([\n",
    "            layers.Dense(self.channels // self.reduction, activation='relu'),\n",
    "            layers.Dense(self.channels)\n",
    "        ])\n",
    "\n",
    "        # === Spatial Attention Convolution ===\n",
    "        # 7x7 conv layer with sigmoid activation to get spatial attention map\n",
    "        self.spatial_conv = layers.Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # ===== CHANNEL ATTENTION =====\n",
    "        # Global average and max pooling to generate descriptors\n",
    "        avg_pool = layers.GlobalAveragePooling2D()(inputs)  # shape: (batch, C)\n",
    "        max_pool = layers.GlobalMaxPooling2D()(inputs)      # shape: (batch, C)\n",
    "\n",
    "        # Apply shared MLP to both\n",
    "        avg_out = self.shared_dense(avg_pool)  # shape: (batch, C)\n",
    "        max_out = self.shared_dense(max_pool)  # shape: (batch, C)\n",
    "\n",
    "        # Combine and apply sigmoid activation to get channel attention weights\n",
    "        channel = layers.Add()([avg_out, max_out])            # shape: (batch, C)\n",
    "        channel = layers.Activation('sigmoid')(channel)\n",
    "        channel = layers.Reshape((1, 1, self.channels))(channel)  # reshape for broadcasting\n",
    "\n",
    "        # Apply channel attention\n",
    "        x = layers.Multiply()([inputs, channel])  # shape: (batch, H, W, C)\n",
    "\n",
    "        # ===== SPATIAL ATTENTION =====\n",
    "        # Average and max pooling along the channel axis\n",
    "        avg_pool_spatial = tf.reduce_mean(x, axis=-1, keepdims=True)  # shape: (batch, H, W, 1)\n",
    "        max_pool_spatial = tf.reduce_max(x, axis=-1, keepdims=True)   # shape: (batch, H, W, 1)\n",
    "\n",
    "        # Concatenate along channel dimension\n",
    "        spatial = layers.Concatenate(axis=-1)([avg_pool_spatial, max_pool_spatial])  # shape: (batch, H, W, 2)\n",
    "\n",
    "        # Apply spatial attention convolution\n",
    "        spatial = self.spatial_conv(spatial)  # shape: (batch, H, W, 1)\n",
    "\n",
    "        # Multiply input with spatial attention map\n",
    "        return layers.Multiply()([x, spatial])  # Final output after CBAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05fc5566-fc01-4154-ad7e-6e10219ef9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load pretrained ResNet50 (without top classifier) ===\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False,        # Exclude final dense classification layer\n",
    "    weights='imagenet',       # Use ImageNet pretrained weights\n",
    "    input_shape=(224, 224, 3) # Input shape for face images\n",
    ")\n",
    "base_model.trainable = False  # Freeze ResNet50 layers to retain pretrained features\n",
    "\n",
    "# === Feature extractor output ===\n",
    "x = base_model.output  # Output of the last convolutional block from ResNet50\n",
    "\n",
    "# === Custom enhancement CNN block + SE + CBAM ===\n",
    "# Add custom Conv2D layer to enhance local features\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "# Apply Squeeze-and-Excitation block for channel attention\n",
    "x = se_block(x)\n",
    "\n",
    "# Apply CBAM block for combined channel + spatial attention\n",
    "x = CBAM()(x)\n",
    "\n",
    "# Additional convolution for refinement\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "# Global pooling to flatten the feature maps\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# === Final dense layer to produce embedding ===\n",
    "# 128-dimensional embedding vector\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "# Apply dropout to prevent overfitting\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "# L2 normalize the embeddings so they lie on a unit hypersphere\n",
    "embedding = layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(x)\n",
    "\n",
    "# === Final embedding model ===\n",
    "# Input: image | Output: 128-d normalized embedding vector\n",
    "embedding_model = tf.keras.Model(inputs=base_model.input, outputs=embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9736131-632f-48b0-a351-2b55ad8c9565",
   "metadata": {},
   "source": [
    "## LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d22c9ddb-fe2e-4f7b-9ad3-8046f20dcf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Triplet Loss Function ===\n",
    "# margin: Hyperparameter that defines how far apart the negative should be from the anchor compared to the positive\n",
    "\n",
    "def triplet_loss(margin=0.2):\n",
    "    \n",
    "    def _loss(y_true, y_pred):\n",
    "        # y_pred shape: (batch_size, 3, embedding_dim)\n",
    "        # Each triplet contains 3 embeddings: [anchor, positive, negative]\n",
    "        \n",
    "        anchor = y_pred[:, 0, :]   # Extract anchor embeddings\n",
    "        positive = y_pred[:, 1, :] # Extract positive embeddings (same identity as anchor)\n",
    "        negative = y_pred[:, 2, :] # Extract negative embeddings (different identity)\n",
    "        \n",
    "        # Calculate squared Euclidean distance between anchor and positive\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "        \n",
    "        # Calculate squared Euclidean distance between anchor and negative\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "        \n",
    "        # Triplet loss = max(pos_dist - neg_dist + margin, 0)\n",
    "        # Encourages the anchor-positive distance to be smaller than anchor-negative by at least the margin\n",
    "        loss = tf.maximum(pos_dist - neg_dist + margin, 0.0)\n",
    "        \n",
    "        # Return average loss over the batch\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return _loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "886c0228-d2b7-48a2-8707-33bb28226f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Custom Accuracy Metric for Triplet Loss ===\n",
    "# Measures how often the positive is closer to the anchor than the negative\n",
    "\n",
    "class TripletAccuracy(tf.keras.metrics.Metric):\n",
    "    \n",
    "    def __init__(self, name=\"triplet_accuracy\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        \n",
    "        # Initialize variables to track correct predictions and total samples\n",
    "        self.correct = self.add_weight(name=\"correct\", initializer=\"zeros\")\n",
    "        self.total = self.add_weight(name=\"total\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Split the predicted embeddings into anchor, positive, and negative\n",
    "        anchor   = y_pred[:, 0, :]  # Shape: (batch_size, embedding_dim)\n",
    "        positive = y_pred[:, 1, :]\n",
    "        negative = y_pred[:, 2, :]\n",
    "\n",
    "        # Calculate squared Euclidean distances\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)  # distance between anchor and positive\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)  # distance between anchor and negative\n",
    "\n",
    "        # Count how many times positive distance is smaller than negative (correct triplets)\n",
    "        correct = tf.reduce_sum(tf.cast(pos_dist < neg_dist, tf.float32))\n",
    "\n",
    "        # Update metric state\n",
    "        self.correct.assign_add(correct)\n",
    "        self.total.assign_add(tf.cast(tf.shape(anchor)[0], tf.float32))  # Add batch size to total\n",
    "\n",
    "    def result(self):\n",
    "        # Return accuracy as correct / total\n",
    "        return self.correct / self.total\n",
    "\n",
    "    def reset_states(self):\n",
    "        # Reset metric state between epochs\n",
    "        self.correct.assign(0.0)\n",
    "        self.total.assign(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1373c5c1-4fce-4d01-8b71-f0dd643d4123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "# === Build Triplet Network ===\n",
    "# Accepts 3 inputs: anchor, positive, and negative images\n",
    "# Uses the same embedding model for all three branches (weight sharing)\n",
    "# Outputs stacked embeddings for calculating triplet loss\n",
    "\n",
    "def build_triplet_model(embedding_model):\n",
    "    # Define 3 input layers for anchor, positive, and negative images\n",
    "    input_anchor = Input(shape=(224, 224, 3), name=\"anchor\")\n",
    "    input_positive = Input(shape=(224, 224, 3), name=\"positive\")\n",
    "    input_negative = Input(shape=(224, 224, 3), name=\"negative\")\n",
    "\n",
    "    # Pass all 3 inputs through the shared embedding model\n",
    "    emb_anchor = embedding_model(input_anchor)\n",
    "    emb_positive = embedding_model(input_positive)\n",
    "    emb_negative = embedding_model(input_negative)\n",
    "\n",
    "    # Stack the 3 embeddings along a new dimension (axis=1)\n",
    "    # Resulting shape: (batch_size, 3, embedding_dim)\n",
    "    stacked_output = Lambda(lambda x: tf.stack(x, axis=1))(\n",
    "        [emb_anchor, emb_positive, emb_negative]\n",
    "    )\n",
    "\n",
    "    # Create and return the final triplet model\n",
    "    return Model(\n",
    "        inputs=[input_anchor, input_positive, input_negative],\n",
    "        outputs=stacked_output\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "718ee315-e884-4a05-b951-f8124aac3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build the Triplet Network ===\n",
    "# `embedding_model` is a base model that encodes input images into feature vectors\n",
    "triplet_net = build_triplet_model(embedding_model)\n",
    "\n",
    "# === Compile the Model ===\n",
    "triplet_net.compile(\n",
    "    optimizer='adam',                       # Adam optimizer for efficient gradient updates\n",
    "    loss=triplet_loss(margin=0.2),          # Custom triplet loss function with margin\n",
    "    metrics=[TripletAccuracy()]             # Custom metric that evaluates how often anchor-positive < anchor-negative\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9f631242-dbd8-43df-bb2c-e8c218d271f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1041s\u001b[0m 16s/step - loss: 0.2369 - triplet_accuracy: 0.4878\n",
      "Epoch 2/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m967s\u001b[0m 16s/step - loss: 0.2144 - triplet_accuracy: 0.5110\n",
      "Epoch 3/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1031s\u001b[0m 16s/step - loss: 0.2128 - triplet_accuracy: 0.4828\n",
      "Epoch 4/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1043s\u001b[0m 17s/step - loss: 0.2163 - triplet_accuracy: 0.4929\n",
      "Epoch 5/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1106s\u001b[0m 17s/step - loss: 0.2149 - triplet_accuracy: 0.5080\n",
      "Epoch 6/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1051s\u001b[0m 17s/step - loss: 0.2114 - triplet_accuracy: 0.4887\n",
      "Epoch 7/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1028s\u001b[0m 16s/step - loss: 0.2123 - triplet_accuracy: 0.4904\n",
      "Epoch 8/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m975s\u001b[0m 16s/step - loss: 0.2056 - triplet_accuracy: 0.5239\n",
      "Epoch 9/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m965s\u001b[0m 16s/step - loss: 0.2013 - triplet_accuracy: 0.5306\n",
      "Epoch 10/10\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1019s\u001b[0m 16s/step - loss: 0.2064 - triplet_accuracy: 0.4948\n"
     ]
    }
   ],
   "source": [
    "# === Train the Triplet Network ===\n",
    "history = triplet_net.fit(\n",
    "    train_triplet_dataset_with_labels,  # Input dataset with ((anchor, positive, negative), dummy_label) format\n",
    "    epochs=10                           # Train the model for 10 full passes over the dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca1eb14e-df0f-4c50-96a1-4b4852df09c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_triplet_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtriplet_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_triplet_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_triplet_accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  # 📊 For plotting graphs\n",
    "\n",
    "# === Plot Training Accuracy and Loss ===\n",
    "plt.plot(history.history['triplet_accuracy'], label='Train Accuracy')  # 📈 Plot triplet accuracy over epochs\n",
    "plt.plot(history.history['loss'], label='Train Loss')                  # 📉 Plot loss over epochs\n",
    "\n",
    "# === Add Plot Labels and Decorations ===\n",
    "plt.xlabel(\"Epochs\")              # 🏷️ X-axis shows number of epochs\n",
    "plt.ylabel(\"Value\")               # 🏷️ Y-axis shows metric value (accuracy or loss)\n",
    "plt.legend()                      # 📌 Display legend for clarity\n",
    "plt.title(\"Training Metrics\")     # 📝 Title of the plot\n",
    "plt.grid(True)                    # 🔳 Add a grid for better readability\n",
    "\n",
    "# === Show the plot ===\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "93aca0ad-47ae-4242-83f8-a5c168f488f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model, image_path):\n",
    "    # === Load and Preprocess the Image ===\n",
    "    image = tf.io.read_file(image_path)                      # 📂 Read image file as raw bytes\n",
    "    image = tf.image.decode_jpeg(image, channels=3)          # 🖼️ Decode JPEG image to tensor with 3 channels (RGB)\n",
    "    image = tf.image.resize(image, [224, 224])               # 📏 Resize image to 224x224 to match model input\n",
    "    image = tf.cast(image, tf.float32) / 255.0               # 🧼 Normalize pixel values to [0, 1]\n",
    "    image = tf.expand_dims(image, axis=0)                    # ➕ Add batch dimension (shape becomes [1, 224, 224, 3])\n",
    "\n",
    "    # === Extract Embedding using the Model ===\n",
    "    embedding = model(image)                                # 🔍 Pass image through model to get embedding vector\n",
    "    return tf.squeeze(embedding).numpy()                    # 🔽 Remove batch dimension and return as NumPy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "95e6e0dc-7cdc-438d-a955-7b59d2a15623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cosine Similarity ===\n",
    "def cosine_similarity(emb1, emb2):\n",
    "    dot = np.dot(emb1, emb2)                               # 🔘 Compute dot product between two embeddings\n",
    "    norm = np.linalg.norm(emb1) * np.linalg.norm(emb2)     # 📏 Compute product of their magnitudes (L2 norms)\n",
    "    return dot / norm                                      # ➗ Return cosine similarity (value between -1 and 1)\n",
    "# === Euclidean Distance ===\n",
    "def euclidean_distance(emb1, emb2):\n",
    "    return np.linalg.norm(emb1 - emb2)                     # 📏 Compute L2 distance (straight-line distance between points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "85bcc1be-70e8-4c7e-96d9-16d70233329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_face(test_embedding, reference_db, threshold=0.5):\n",
    "    best_match = None\n",
    "    best_score = -1  # ✅ Initialize with worst similarity score (cosine similarity ranges from -1 to 1)\n",
    "\n",
    "    # 🔍 Loop through each entry in the reference database\n",
    "    for name, ref_embedding in reference_db.items():\n",
    "        score = cosine_similarity(test_embedding, ref_embedding)  # 📏 Compute similarity between test and reference\n",
    "        if score > best_score:\n",
    "            best_match = name     # 🎯 Update best match if this score is better\n",
    "            best_score = score    # 📝 Track the highest similarity\n",
    "\n",
    "    # ✅ Return best match if similarity exceeds the threshold\n",
    "    if best_score >= threshold:\n",
    "        return best_match, best_score\n",
    "    else:\n",
    "        return \"Unknown\", best_score   # ❌ Return 'Unknown' if no match is confident enough\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e126ad-c5f0-4071-89f6-0e44f505f031",
   "metadata": {},
   "source": [
    "## SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4af2095f-8316-4788-9707-c3b57cd2f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_centroids(image_paths_by_class, model):\n",
    "    class_centroids = {}  # 🧠 Dictionary to store the centroid (mean embedding) for each class\n",
    "\n",
    "    # 🔁 Loop through each class and its corresponding image paths\n",
    "    for class_name, image_paths in image_paths_by_class.items():\n",
    "        # 📌 Compute embeddings for each image in the class\n",
    "        embeddings = [get_embedding(model, p) for p in image_paths]\n",
    "\n",
    "        # 📊 Compute the mean embedding (centroid) for the class\n",
    "        class_centroids[class_name] = np.mean(embeddings, axis=0)\n",
    "\n",
    "    return class_centroids  # 🔁 Return dictionary of class: centroid pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1827391f-f4f1-420f-8d81-8a582f314e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(test_path, model, reference_db):\n",
    "    # 📷 Step 1: Generate embedding for the test image\n",
    "    test_embedding = get_embedding(model, test_path)\n",
    "\n",
    "    # 📏 Step 2: Compute cosine similarity between test image and all reference embeddings\n",
    "    similarities = {\n",
    "        class_name: cosine_similarity(test_embedding, emb)\n",
    "        for class_name, emb in reference_db.items()\n",
    "    }\n",
    "\n",
    "    # 🏆 Step 3: Return the class name with the highest similarity score (Top-1 match)\n",
    "    return max(similarities.items(), key=lambda x: x[1])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0b1ab7bd-66e0-4d1a-9bee-510d57c3edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 📦 Create a dictionary to store clean (non-distorted) image paths per class/person\n",
    "reference_data = defaultdict(list)\n",
    "\n",
    "# 🔁 Loop over all image paths from the val dataset\n",
    "for path in val_image_paths:\n",
    "    # 🚫 Filter out distorted images (case-insensitive check)\n",
    "    if \"distortion\" not in path.lower():\n",
    "        # 🏷️ Extract class/label name from the parent folder name (e.g., 'Person_01')\n",
    "        label = os.path.basename(os.path.dirname(path))\n",
    "\n",
    "        # 📥 Group image path under its class label\n",
    "        reference_data[label].append(path)\n",
    "\n",
    "# ✅ Convert defaultdict to a standard Python dictionary for usability\n",
    "reference_data = dict(reference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "701552f3-9f8e-4f81-991a-9b3b55a5c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_db = {}\n",
    "\n",
    "# 🔁 Iterate through each label (person/class) and their associated clean images\n",
    "for label, img_list in reference_data.items():\n",
    "    # 📷 Select the first clean image from the list for this class\n",
    "    img_path = img_list[0]\n",
    "\n",
    "    # 🔐 Generate the embedding vector using the trained embedding model\n",
    "    reference_db[label] = get_embedding(embedding_model, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "05776242-ebfd-46f9-ad0a-2703a45eee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 009_frontal\n"
     ]
    }
   ],
   "source": [
    "# 🔀 Randomly select one image path from the validation dataset\n",
    "test_path = random.choice(val_image_paths)\n",
    "\n",
    "# 🧠 Predict the class of the selected image using the trained embedding model\n",
    "#     - Computes embedding for test image\n",
    "#     - Compares with reference_db (known class embeddings)\n",
    "#     - Returns the class with highest cosine similarity\n",
    "predicted_class = predict_class(test_path, embedding_model, reference_db)\n",
    "\n",
    "# 🖨️ Output the predicted class label to the console\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e0be4174-5156-452c-abef-c86e431d179c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17712\\3666164876.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtop1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_macro\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# 📊 === Run Evaluation ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mreference_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_class_centroids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreference_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mtop1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_macro\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_image_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Top-1 Accuracy: {top1:.4f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17712\\1972808914.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(image_paths_by_class, model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_class_centroids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_paths_by_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mclass_centroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_paths\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimage_paths_by_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mclass_centroids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclass_centroids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17712\\1972808914.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_class_centroids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_paths_by_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mclass_centroids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_paths\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimage_paths_by_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17712\\2056916969.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(model, image_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Add batch dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Get embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    944\u001b[0m                     \u001b[1;34m\"layers will not see the mask.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m                 )\n\u001b[0;32m    946\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m             \u001b[1;31m# Destroy call context if we created it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0;32m     49\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             )\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# Plain flow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\models\\functional.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_keras_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         outputs = self._run_through_graph(\n\u001b[0m\u001b[0;32m    184\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         )\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\ops\\function.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcall_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\models\\functional.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    639\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0moperation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_has_training_arg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         ):\n\u001b[0;32m    642\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"training\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0moperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    944\u001b[0m                     \u001b[1;34m\"layers will not see the mask.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m                 )\n\u001b[0;32m    946\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m             \u001b[1;31m# Destroy call context if we created it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0;32m     49\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             )\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# Plain flow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m         outputs = ops.batch_normalization(\n\u001b[0m\u001b[0;32m    279\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0mvariance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\ops\\nn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, mean, variance, axis, offset, scale, epsilon)\u001b[0m\n\u001b[0;32m   2201\u001b[0m         return BatchNorm(axis, epsilon).symbolic_call(\n\u001b[0;32m   2202\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2203\u001b[0m         )\n\u001b[0;32m   2204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2205\u001b[1;33m     return backend.nn.batch_normalization(\n\u001b[0m\u001b[0;32m   2206\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2207\u001b[0m     )\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, mean, variance, axis, offset, scale, epsilon)\u001b[0m\n\u001b[0;32m    866\u001b[0m             \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m             \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m     return tf.nn.batch_normalization(\n\u001b[0m\u001b[0;32m    871\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m         \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m         \u001b[0mvariance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[0;32m   1480\u001b[0m   \"\"\"\n\u001b[0;32m   1481\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"batchnorm\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m     \u001b[0minv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvariance_epsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m       \u001b[0minv\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1485\u001b[0m     \u001b[1;31m# Note: tensorflow/contrib/quantize/python/fold_batch_norms.py depends on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m     \u001b[1;31m# the precise order of ops that are generated by the expression below.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m     return x * math_ops.cast(inv, x.dtype) + math_ops.cast(\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\tensorflow\\python\\framework\\override_binary_operator.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;31m# object that can implement the operator with knowledge of itself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# and the tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_math_operator_overrides.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         ),\n\u001b[0;32m     74\u001b[0m         \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     )  # pylint: disable=protected-access\n\u001b[1;32m---> 76\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1744\u001b[0m     new_vals = gen_sparse_ops.sparse_dense_cwise_mul(y.indices, y.values,\n\u001b[0;32m   1745\u001b[0m                                                      y.dense_shape, x, name)\n\u001b[0;32m   1746\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1748\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m    \u001b[1;33m*\u001b[0m \u001b[0mInvalidArgumentError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mWhen\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mincompatible\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m   \"\"\"\n\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniforge3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6825\u001b[0m         _ctx, \"Mul\", name, x, y)\n\u001b[0;32m   6826\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6827\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6828\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6829\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6830\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6831\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6832\u001b[0m       return mul_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "reference_embeddings = compute_class_centroids(reference_data, embedding_model)\n",
    "\n",
    "def evaluate(model, val_data, reference_embeddings):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for path in val_data:\n",
    "        label = os.path.basename(os.path.dirname(path))  # True label from folder name\n",
    "        pred = predict_class(path, model, reference_embeddings)\n",
    "\n",
    "        y_true.append(label)\n",
    "        y_pred.append(pred)\n",
    "\n",
    "    # 🧮 Top-1 Accuracy\n",
    "    top1 = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "\n",
    "    # 🧮 Macro F1-score\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    return top1, f1_macro\n",
    "\n",
    "# 📊 === Run Evaluation ===\n",
    "reference_embeddings = compute_class_centroids(reference_data, embedding_model)\n",
    "top1, f1_macro = evaluate(embedding_model, val_image_paths, reference_embeddings)\n",
    "\n",
    "print(f\"Top-1 Accuracy: {top1:.4f}\")\n",
    "print(f\"Macro-averaged F1-Score: {f1_macro:.4f}\")\n",
    "print(f\"Top-1 Accuracy: {top1:.4f}\")\n",
    "print(f\"Macro-averaged F1-Score: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fee91-4dab-487a-896e-2e477b6c80fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📋 Print the architecture of the Triplet Network model\n",
    "triplet_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d714cd-4925-4b97-ace9-953b7aa95b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 Show the architecture of the embedding model\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78add0-dd82-436c-8deb-20850858905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Loop through each layer in the triplet model\n",
    "for layer in triplet_net.layers:\n",
    "    print(f\"Layer: {layer.name}\")  # 🧩 Print the name of the current layer\n",
    "\n",
    "    # 🔢 Get the weights (if any) associated with this layer\n",
    "    weights = layer.get_weights()\n",
    "\n",
    "    # 📏 Loop through each weight tensor (kernel, bias, etc.)\n",
    "    for i, w in enumerate(weights):\n",
    "        print(f\"  Weight {i}: shape={w.shape}\")  # 📐 Print shape of each weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ccb8d80c-4d61-451b-bb52-aca94792a3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# 💾 Save the entire triplet network model (architecture + weights + optimizer state)\n",
    "triplet_net.save(\"triplet_network_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1a3e6706-f77d-46c4-8e0c-c32996f4ef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# 💾 Save the face embedding model (ResNet50 + custom layers) for future use\n",
    "# This includes the architecture, weights, and training config (if any)\n",
    "embedding_model.save(\"face_embedding_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70444e34-ed24-476d-9f63-5b87f62116e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
